{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset https://drive.google.com/drive/u/1/folders/1rkMSP55j3spt3xrvSSQMWborBRa75iXL\n",
        "\n",
        "Model https://drive.google.com/drive/u/1/folders/1nJD8oGg4kgwmypMoE4GHhTc-efNAr-cd"
      ],
      "metadata": {
        "id": "30Mt1rjtpkrE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUTfD2Wbqdht"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_current_env():\n",
        "    '''獲取當前目標的絕對路徑，且添加 Python 環境'''\n",
        "\n",
        "    import os\n",
        "    # 獲取根目錄\n",
        "    try:  # colab 目錄\n",
        "        from google.colab import drive\n",
        "        root = '/content/drive'  # colab 訓練\n",
        "        drive.mount(root)  # 掛載雲端硬碟\n",
        "        root = f'{root}/MyDrive'\n",
        "    except:\n",
        "        root = '.'  # 本地目錄\n",
        "    # 添加當前路徑為 Python 包所在環境\n",
        "    # 保證 colab 可以獲取自訂義的 .py 文件\n",
        "    os.chdir(root)\n",
        "\n",
        "load_current_env()"
      ],
      "metadata": {
        "id": "ojMhJd1nqeO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.applications import ResNet152\n",
        "\n",
        "def mkdir(out_dir):\n",
        "    out_dir = Path(out_dir)\n",
        "    if not out_dir.exists():\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def load_current_env():\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        root = '/content/drive'\n",
        "        drive.mount(root)\n",
        "        root = f'{root}/MyDrive'\n",
        "    except:\n",
        "        root = '.'\n",
        "    os.chdir(root)\n",
        "\n",
        "def load_data(train_data_dir,\n",
        "              validation_data_dir,\n",
        "              test_data_dir,\n",
        "              batch_size,\n",
        "              target_size=(224, 224),\n",
        "              class_mode='binary'):\n",
        "\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        preprocessing_function=preprocess_input,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "    validation_datagen = ImageDataGenerator(\n",
        "        preprocessing_function=preprocess_input)\n",
        "\n",
        "    test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
        "                                                        target_size=target_size,\n",
        "                                                        batch_size=batch_size,\n",
        "                                                        class_mode=class_mode)\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(validation_data_dir,\n",
        "                                                                  shuffle=False,\n",
        "                                                                  target_size=target_size,\n",
        "                                                                  batch_size=batch_size,\n",
        "                                                                  class_mode=class_mode)\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(test_data_dir,\n",
        "                                                      target_size=target_size,\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      interpolation='bicubic',\n",
        "                                                      class_mode=class_mode,\n",
        "                                                      shuffle=False)\n",
        "    return train_generator, validation_generator, test_generator\n",
        "\n",
        "def create_model(base_model):\n",
        "    model = Sequential()\n",
        "    model.add(base_model)\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(1024, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    base_model.trainable = False\n",
        "    return model\n",
        "\n",
        "def create_history(model, epochs, batch_size, train_generator, validation_generator, checkpoint_filepath='../checkpoint'):\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        mode='auto', save_freq='epoch',\n",
        "        save_best_only=True)\n",
        "\n",
        "    history = model.fit(x=train_generator,\n",
        "                        batch_size=train_generator.n // batch_size,\n",
        "                        epochs=epochs,\n",
        "                        validation_data=validation_generator,\n",
        "                        validation_steps=validation_generator.n // batch_size,\n",
        "                        callbacks=[model_checkpoint_callback])\n",
        "    return history\n",
        "\n",
        "def write_csv_result(save_dir, model_name, accs, val_accs, losses, val_losses):\n",
        "    with open(f\"{save_dir}/{model_name}_train_acc.csv\", \"w\") as f:\n",
        "        [f.write(str(acc)+'\\n') for acc in accs]\n",
        "    with open(f\"{save_dir}/{model_name}_train_loss.csv\", \"w\") as f:\n",
        "        [f.write(str(loss)+'\\n') for loss in losses]\n",
        "    with open(f\"{save_dir}/{model_name}_val_acc.csv\", \"w\") as f:\n",
        "        [f.write(str(acc)+'\\n') for acc in val_accs]\n",
        "    with open(f\"{save_dir}/{model_name}_val_loss.csv\", \"w\") as f:\n",
        "        [f.write(str(loss)+'\\n') for loss in val_losses]\n",
        "\n",
        "def plot_history(history, save_dir, model_name):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    write_csv_result(save_dir, model_name, acc, val_acc, loss, val_loss)\n",
        "    plt.plot(acc, 'b', label='Training acc')\n",
        "    plt.plot(val_acc, 'r--', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.savefig(\n",
        "        f'{save_dir}/{model_name}_training_validation_accuracy.png')\n",
        "    plt.figure()\n",
        "\n",
        "    plt.plot(loss, 'b', label='Training loss')\n",
        "    plt.plot(val_loss, 'r--', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(\n",
        "        f'{save_dir}/{model_name}_training_validation_loss.png')\n",
        "    plt.show()\n",
        "\n",
        "root = '/content/drive/My Drive/'\n",
        "train_data_dir = f'{root}/myData3200/train'\n",
        "val_data_dir = f'{root}/myData3200/validation'\n",
        "test_data_dir = f'{root}/myData3200/test'\n",
        "\n",
        "target_size = 224, 224\n",
        "batch_size = 32\n",
        "class_mode = 'binary'\n",
        "\n",
        "train_gen, val_gen, test_gen = load_data(train_data_dir,\n",
        "                                        val_data_dir,\n",
        "                                        test_data_dir,\n",
        "                                        batch_size,\n",
        "                                        target_size,\n",
        "                                        class_mode)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
        "\n",
        "resnet152_model = create_model(ResNet152(weights='imagenet', include_top=False))\n",
        "resnet152_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "model_save_dir = '/content/drive/My Drive/resnet152_Adam_DefaultValue'\n",
        "mkdir(model_save_dir)\n",
        "\n",
        "save_dir = '/content/drive/My Drive/resnet152_Adam_DefaultValue'\n",
        "mkdir(save_dir)\n",
        "\n",
        "model_name = 'resnet152'\n",
        "\n",
        "checkpoint_filepath = f'{save_dir}/{model_name}/'+ 'weights.{epoch:03d}-{val_loss:.7f}.h5'\n",
        "\n",
        "resnet152_history = create_history(\n",
        "    resnet152_model, epochs, batch_size,\n",
        "    train_gen, val_gen,\n",
        "    checkpoint_filepath)\n",
        "\n",
        "plot_history(resnet152_history, save_dir, model_name)\n",
        "resnet152_model.save(f'{model_save_dir}/{model_name}.h5')\n",
        "\n",
        "def test_result(model, test_generator, batch_size):\n",
        "    results = model.evaluate(x=test_generator,\n",
        "                                         batch_size=batch_size,\n",
        "                                        return_dict=True)\n",
        "    return results\n",
        "\n",
        "resnet152_results = test_result(resnet152_model, test_gen, batch_size)\n"
      ],
      "metadata": {
        "id": "vSnjBno7uiHc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}